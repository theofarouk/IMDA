{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theofarouk/IMDA/blob/main/Intro_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bafc2a2c-009e-460e-a06e-eae84178a122",
      "metadata": {
        "id": "bafc2a2c-009e-460e-a06e-eae84178a122"
      },
      "source": [
        "# **Introduction to Diffusion Models for Text-to-Image Generation**\n",
        "\n",
        "---\n",
        "\n",
        "### **<p style=\"text-align: center; text-decoration: underline;\">Introduction to Diffusion Models</p>**\n",
        "# **<p style=\"text-align: center;\">Text-to-Image Generation with Diffusion Models</p>**\n",
        "\n",
        "---\n",
        "\n",
        "> Tutor: *Omar IKNE*\n",
        "\n",
        "> Master 2, IMT Nord Europe\n",
        "\n",
        "---\n",
        "\n",
        "### ■ **Overview**\n",
        "\n",
        "In this notebook, we will explore the fascinating world of diffusion models for text-to-image generation. Diffusion models have revolutionized image generation by learning to reverse a gradual noising process, creating highly realistic images from random noise. We'll build a simple diffusion model from scratch and train it on a small dataset to understand the fundamental concepts.\n",
        "\n",
        "**Main Task: Text-to-Image Generation**\n",
        "\n",
        "![Diffusion Process](https://imgs.search.brave.com/HQ6f8BLM8vHBXJWwYv7MdMbq6ddhGU-pZOKWQnYCP58/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9pbWFn/ZXMucHJpc21pYy5p/by9lbmNvcmQvZjAz/ZWQ5OWItOTBkMi00/NDQxLTg1MzctYThh/YTBhZjIzMjQ2X2lt/YWdlMTIucG5nP2F1/dG89Y29tcHJlc3Ms/Zm9ybWF0)\n",
        "\n",
        "### ■ **Contents**\n",
        "\n",
        "- [1. Preliminaries](#section1)\n",
        "- [2. Understanding Diffusion Models](#section2)\n",
        "- [3. Dataset Preparation](#section3)\n",
        "- [4. Building the Diffusion Model](#section4)\n",
        "- [5. Training the Model](#section5)\n",
        "- [6. Text-to-Image Generation](#section6)\n",
        "- [7. Model Evaluation](#section7)\n",
        "\n",
        "---\n",
        "\n",
        "### ■ **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6000ca-6eb5-426c-a7b0-8c678f3d9607",
      "metadata": {
        "tags": [],
        "id": "eb6000ca-6eb5-426c-a7b0-8c678f3d9607"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# !pip install torch torchvision matplotlib pillow tqdm numpy\n",
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b18d2b84-7e06-4bcd-b3f8-51d4c864d7a1",
      "metadata": {
        "tags": [],
        "id": "b18d2b84-7e06-4bcd-b3f8-51d4c864d7a1"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Basic dependencies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def seed_everything(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9f9e835-1370-4b49-9332-1c80cee0e931",
      "metadata": {
        "id": "a9f9e835-1370-4b49-9332-1c80cee0e931"
      },
      "source": [
        "\n",
        "### ■ **<a name=\"section1\">1. Preliminaries</a>** [(&#8593;)](#content)\n",
        "\n",
        "Diffusion models are a class of generative models that have shown remarkable performance in image generation tasks. The core idea is to learn a process that gradually transforms random noise into realistic images.\n",
        "\n",
        "#### **Key Concepts:**\n",
        "\n",
        "1. **Forward Process (Diffusion):** Gradually add noise to an image until it becomes pure Gaussian noise\n",
        "2. **Reverse Process (Denoising):** Learn to reverse the noise addition process to generate images from noise\n",
        "3. **Noise Schedule:** Controls how much noise is added at each step\n",
        "4. **Text Conditioning:** Using text prompts to guide the image generation process\n",
        "\n",
        "#### **Mathematical Foundation:**\n",
        "\n",
        "The diffusion process can be described as a Markov chain:\n",
        "- Forward: $q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)$\n",
        "- Reverse: $p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$\n",
        "\n",
        "Where $\\beta_t$ is the noise schedule and $\\theta$ represents the model parameters.\n",
        "\n",
        "---\n",
        "\n",
        "### ■ **<a name=\"section2\">2. Understanding Diffusion Models</a>** [(&#8593;)](#content)\n",
        "\n",
        "Let's first understand the core concepts through implementation before building our complete model.\n",
        "\n",
        "#### **Question 1: What is the forward diffusion process?**\n",
        "\n",
        "The forward process gradually adds Gaussian noise to an image over multiple timesteps.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def download_image(image_url, file_dir):\n",
        "    response = requests.get(image_url)\n",
        "    if response.status_code == 200:\n",
        "        directory = os.path.dirname(file_dir)\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "        with open(file_dir, \"wb\") as fp:\n",
        "            fp.write(response.content)\n",
        "        print(\"Image downloaded successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to download the image. Status code: {response.status_code}\")\n",
        "\n",
        "# Download an example image\n",
        "image_url = \"https://img-9gag-fun.9cache.com/photo/aPg3MoB_460s.jpg\"\n",
        "file_dir = \"./image.jpg\"\n",
        "download_image(image_url, file_dir)"
      ],
      "metadata": {
        "id": "Sz0CmQrmco3i"
      },
      "id": "Sz0CmQrmco3i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "382d0acf-ce38-4d7a-9c96-43ca3ffafd1c",
      "metadata": {
        "tags": [],
        "id": "382d0acf-ce38-4d7a-9c96-43ca3ffafd1c"
      },
      "outputs": [],
      "source": [
        "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
        "    \"\"\"Linear noise schedule for the diffusion process\"\"\"\n",
        "    return torch.linspace(start, end, timesteps)\n",
        "\n",
        "def forward_diffusion_sample(x_0, t, betas, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Sample from q(x_t | x_0) using the reparameterization trick\n",
        "\n",
        "    Args:\n",
        "        x_0: Original image (batch_size, channels, height, width)\n",
        "        t: Timestep (batch_size,)\n",
        "        betas: Noise schedule (timesteps,)\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        x_t: Noisy image at timestep t\n",
        "        noise: The noise that was added\n",
        "    \"\"\"\n",
        "    # Extract sqrt(alpha_bar) and sqrt(1-alpha_bar) for timestep t\n",
        "    t = t.to(device)\n",
        "    sqrt_alphas_cumprod = torch.sqrt(1. - betas).cumprod(dim=0).to(device)\n",
        "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - sqrt_alphas_cumprod ** 2)\n",
        "\n",
        "    # Gather the appropriate values for timestep t\n",
        "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod[t].to(device)\n",
        "    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t].to(device)\n",
        "\n",
        "    # Reshape for broadcasting\n",
        "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t[:, None, None, None]\n",
        "    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t[:, None, None, None]\n",
        "\n",
        "    # Sample noise\n",
        "    noise = torch.randn_like(x_0).to(device)\n",
        "\n",
        "    # Forward diffusion: x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1-alpha_bar_t) * epsilon\n",
        "    x_t = sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "    return x_t, noise\n",
        "\n",
        "# Let's test the forward diffusion process\n",
        "def visualize_forward_process():\n",
        "    \"\"\"Visualize the forward diffusion process on a sample image\"\"\"\n",
        "    # Load a sample image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "    ])\n",
        "\n",
        "    # Use a sample image (you can replace this with any image)\n",
        "    sample_image = torch.randn(1, 3, 64, 64)  # Random image for demonstration\n",
        "\n",
        "    # Set up diffusion parameters\n",
        "    timesteps = 1000\n",
        "    betas = linear_beta_schedule(timesteps)\n",
        "\n",
        "    # Select specific timesteps to visualize\n",
        "    viz_timesteps = [0, 50, 100, 200, 500, 999]\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(viz_timesteps), figsize=(15, 3))\n",
        "\n",
        "    for i, t in enumerate(viz_timesteps):\n",
        "        x_t, noise = forward_diffusion_sample(sample_image, torch.tensor([t]), betas)\n",
        "\n",
        "        # Denormalize for visualization\n",
        "        img = x_t[0].permute(1, 2, 0)\n",
        "        img = (img * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f't = {t}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Let's test the forward diffusion process\n",
        "def visualize_forward_process_image(image_path):\n",
        "    \"\"\"Visualize the forward diffusion process on a sample image\"\"\"\n",
        "    # Load a sample image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "    ])\n",
        "\n",
        "    # Use a sample image (you can replace this with any image)\n",
        "    sample_image = Image.open(image_path)\n",
        "    sample_image = transforms.ToTensor()(sample_image)\n",
        "\n",
        "    # Set up diffusion parameters\n",
        "    timesteps = 1000\n",
        "    betas = linear_beta_schedule(timesteps)\n",
        "\n",
        "    # Select specific timesteps to visualize\n",
        "    viz_timesteps = [0, 50, 100, 200, 500, 999]\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(viz_timesteps), figsize=(15, 3))\n",
        "\n",
        "    for i, t in enumerate(viz_timesteps):\n",
        "        x_t, noise = forward_diffusion_sample(sample_image, torch.tensor([t]), betas)\n",
        "\n",
        "        # Denormalize for visualization\n",
        "        img = x_t[0].permute(1, 2, 0)\n",
        "        # img = (img * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f't = {t}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# run the visualization\n",
        "image_path = './image.jpg'\n",
        "visualize_forward_process_image(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "498c4947-96ef-4e0d-a0a9-2a2df27c38d3",
      "metadata": {
        "id": "498c4947-96ef-4e0d-a0a9-2a2df27c38d3"
      },
      "source": [
        "**Question 1.1:** What happens to the image as we increase the timestep `t`?\n",
        "\n",
        "L'image devient progressivement plus bruitée jusqu'à devenir du bruit gaussien pur à t=999. Plus t augmente, plus l'image originale est dégradée par l'ajout de bruit\n",
        "\n",
        "**Question 1.2:** Why do we use a noise schedule instead of adding the same amount of noise at each step?\n",
        "\n",
        "Le noise schedule permet un contrôle progressif du processus de diffusion. Un schedule linéaire ou cosinus assure une transition douce de l'image originale vers le bruit, facilitant l'apprentissage du processus inverse.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Question 2: How does the reverse process work?**\n",
        "\n",
        "The reverse process learns to denoise images. We train a neural network (e.g., UNet) to predict the noise that was added.\n",
        "\n",
        "\n",
        "![unet](https://i0.wp.com/eviltux.com/wp-content/uploads/2024/08/1.-UNet_What-Is-It-1.png?w=1000&ssl=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd7386f-3e19-4e99-8268-14d4e9db46fc",
      "metadata": {
        "tags": [],
        "id": "5cd7386f-3e19-4e99-8268-14d4e9db46fc"
      },
      "outputs": [],
      "source": [
        "class SimpleUNet(nn.Module):\n",
        "    \"\"\"A simplified U-Net architecture for diffusion models\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=3, base_channels=64):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "\n",
        "        # Encoder (Downsampling)\n",
        "        self.enc1 = self._block(in_channels, base_channels)\n",
        "        self.enc2 = self._block(base_channels, base_channels * 2)\n",
        "        self.enc3 = self._block(base_channels * 2, base_channels * 4)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self._block(base_channels * 4, base_channels * 8)\n",
        "\n",
        "        # Decoder (Upsampling)\n",
        "        self.dec3 = self._block(base_channels * 12, base_channels * 4)  # Skip connection\n",
        "        self.dec2 = self._block(base_channels * 6, base_channels * 2)   # Skip connection\n",
        "        self.dec1 = self._block(base_channels * 3, base_channels)       # Skip connection\n",
        "\n",
        "        # Final convolution\n",
        "        self.final_conv = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
        "\n",
        "        # Pooling and upsample\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "    def _block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        e3 = self.enc3(self.pool(e2))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool(e3))\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d3 = self.dec3(torch.cat([self.upsample(bottleneck), e3], dim=1))\n",
        "        d2 = self.dec2(torch.cat([self.upsample(d3), e2], dim=1))\n",
        "        d1 = self.dec1(torch.cat([self.upsample(d2), e1], dim=1))\n",
        "\n",
        "        return self.final_conv(d1)\n",
        "\n",
        "# Test the model\n",
        "def test_unet():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SimpleUNet().to(device)\n",
        "\n",
        "    # Create a dummy input\n",
        "    dummy_input = torch.randn(4, 3, 64, 64).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        output = model(dummy_input)\n",
        "\n",
        "    print(f\"Input shape: {dummy_input.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "test_unet()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "726d53c6-16b7-4d43-be6f-dad235113f35",
      "metadata": {
        "id": "726d53c6-16b7-4d43-be6f-dad235113f35"
      },
      "source": [
        "\n",
        "**Question 2.1:** What is the purpose of the U-Net architecture in diffusion models?\n",
        "\n",
        "U-Net prédit le bruit ajouté à l'image à chaque timestep. Son architecture encoder-decoder avec skip connections préserve les détails spatiaux nécessaires pour un débruitage précis\n",
        "\n",
        "**Question 2.2:** Why are skip connections important in the U-Net architecture?\n",
        "\n",
        "Les skip connections permettent de préserver l'information haute résolution du encoder vers le decoder, évitant la perte de détails fins lors du débruitage\n",
        "---\n",
        "\n",
        "### ■ **<a name=\"section3\">3. Dataset Preparation</a>** [(&#8593;)](#content)\n",
        "\n",
        "We'll use a simple dataset for our text-to-image generation task. Let's use the [**CIFAR-10**](https://www.cs.toronto.edu/~kriz/cifar.html) dataset with simple text descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9328aec-3922-4321-98a0-887d3f524a24",
      "metadata": {
        "tags": [],
        "id": "a9328aec-3922-4321-98a0-887d3f524a24"
      },
      "outputs": [],
      "source": [
        "class CIFAR10WithCaptions(torch.utils.data.Dataset):\n",
        "    \"\"\"CIFAR-10 dataset with simple text captions\"\"\"\n",
        "\n",
        "    def __init__(self, train=True, image_size=64):\n",
        "        self.cifar10 = torchvision.datasets.CIFAR10(\n",
        "            root='./data', train=train, download=True,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize((image_size, image_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        # Simple text descriptions for CIFAR-10 classes\n",
        "        self.class_descriptions = {\n",
        "            0: \"an airplane flying in the sky\",\n",
        "            1: \"a car on the road\",\n",
        "            2: \"a bird perched on a branch\",\n",
        "            3: \"a cat sitting on the floor\",\n",
        "            4: \"a deer in the forest\",\n",
        "            5: \"a dog playing in the park\",\n",
        "            6: \"a frog near the pond\",\n",
        "            7: \"a horse in the field\",\n",
        "            8: \"a ship on the ocean\",\n",
        "            9: \"a truck on the highway\"\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cifar10)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.cifar10[idx]\n",
        "        caption = self.class_descriptions[label]\n",
        "\n",
        "        return image, caption, label\n",
        "\n",
        "def visualize_dataset_samples():\n",
        "    \"\"\"Visualize some samples from our dataset\"\"\"\n",
        "    dataset = CIFAR10WithCaptions(train=True)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "\n",
        "    for i in range(10):\n",
        "        image, caption, label = dataset[i]\n",
        "\n",
        "        # Denormalize image\n",
        "        img = image.permute(1, 2, 0)\n",
        "        img = (img * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "        ax = axes[i // 5, i % 5]\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(caption, fontsize=8)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize dataset samples\n",
        "visualize_dataset_samples()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "340b493c-c9d4-4861-a003-ad63d7b40ed1",
      "metadata": {
        "id": "340b493c-c9d4-4861-a003-ad63d7b40ed1"
      },
      "source": [
        "\n",
        "**Question 3.1:** Why did we choose CIFAR-10 for this introductory lab?\n",
        "\n",
        "CIFAR-10 est idéal pour l'apprentissage car il contient des images 32x32 simples, un dataset compact (60K images), et 10 classes bien définies permettant des descriptions textuelles simples\n",
        "\n",
        "**Question 3.2:** What are the advantages and limitations of using simple text descriptions?\n",
        "\n",
        "**Avantages:** Simplicité d'implémentation, correspondance directe classe-description.\n",
        "**Limitations:** Descriptions peu variées, pas de nuances linguistiques, conditionnement textuel limité.\n",
        "\n",
        "---\n",
        "\n",
        "### ■ **<a name=\"section4\">4. Building the Diffusion Model</a>** [(&#8593;)](#content)\n",
        "\n",
        "Now let's build our complete text-conditioned diffusion model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d44b92d-d5db-4844-9755-9344d0fbdb6b",
      "metadata": {
        "tags": [],
        "id": "2d44b92d-d5db-4844-9755-9344d0fbdb6b"
      },
      "outputs": [],
      "source": [
        "class SimpleTextEncoder(nn.Module):\n",
        "    \"\"\"A simple text encoder for our captions\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=1000, embed_dim=512, hidden_dim=512):\n",
        "        super(SimpleTextEncoder, self).__init__()\n",
        "\n",
        "        # Simple embedding based on word positions\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_indices):\n",
        "        # text_indices shape: [batch_size]\n",
        "        embeddings = self.embedding(text_indices)  # [batch_size, embed_dim]\n",
        "        return self.fc(embeddings)  # [batch_size, hidden_dim]\n",
        "\n",
        "# Fix the diffusion model to properly handle text conditioning\n",
        "class TextConditionedDiffusionModel(nn.Module):\n",
        "    \"\"\"Diffusion model with text conditioning\"\"\"\n",
        "\n",
        "    def __init__(self, timesteps=1000, image_size=64, text_embed_dim=512):\n",
        "        super(TextConditionedDiffusionModel, self).__init__()\n",
        "\n",
        "        self.timesteps = timesteps\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # Noise schedule\n",
        "        self.betas = linear_beta_schedule(timesteps)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
        "\n",
        "        # U-Net for denoising\n",
        "        self.unet = SimpleUNet(in_channels=3 + text_embed_dim)  # Add text channels\n",
        "\n",
        "        # Timestep embedding\n",
        "        self.timestep_embedding = nn.Sequential(\n",
        "            nn.Linear(1, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, text_embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, text_embeddings):\n",
        "        \"\"\"\n",
        "        Forward pass of the diffusion model\n",
        "\n",
        "        Args:\n",
        "            x: Noisy images [batch_size, 3, height, width]\n",
        "            t: Timesteps [batch_size]\n",
        "            text_embeddings: Text embeddings [batch_size, embed_dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Add timestep information\n",
        "        t_embed = self.timestep_embedding(t.unsqueeze(1).float())  # [batch_size, text_embed_dim]\n",
        "\n",
        "        # Combine timestep and text information\n",
        "        conditioning = t_embed + text_embeddings  # [batch_size, text_embed_dim]\n",
        "\n",
        "        # Reshape conditioning to spatial dimensions and concatenate with image\n",
        "        conditioning_spatial = conditioning.unsqueeze(-1).unsqueeze(-1)  # [batch_size, text_embed_dim, 1, 1]\n",
        "        conditioning_spatial = conditioning_spatial.repeat(1, 1, self.image_size, self.image_size)  # [batch_size, text_embed_dim, H, W]\n",
        "\n",
        "        # Concatenate conditioning with noisy image\n",
        "        x_conditioned = torch.cat([x, conditioning_spatial], dim=1)  # [batch_size, 3 + text_embed_dim, H, W]\n",
        "\n",
        "        # Predict noise\n",
        "        predicted_noise = self.unet(x_conditioned)\n",
        "\n",
        "        return predicted_noise\n",
        "\n",
        "    def sample_timesteps(self, batch_size):\n",
        "        \"\"\"Sample random timesteps for training\"\"\"\n",
        "        return torch.randint(0, self.timesteps, (batch_size,))\n",
        "\n",
        "\n",
        "# Let's test the model\n",
        "def test_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = TextConditionedDiffusionModel().to(device)\n",
        "    text_encoder = SimpleTextEncoder().to(device)\n",
        "\n",
        "    batch_size = 4\n",
        "\n",
        "    # Dummy inputs\n",
        "    x = torch.randn(batch_size, 3, 64, 64).to(device)\n",
        "    t = torch.randint(0, 1000, (batch_size,)).to(device)\n",
        "    text_indices = torch.randint(0, 10, (batch_size,)).to(device)\n",
        "\n",
        "    # Get text embeddings\n",
        "    text_embeddings = text_encoder(text_indices)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        predicted_noise = model(x, t, text_embeddings)\n",
        "\n",
        "    print(f\"Noisy image shape: {x.shape}\")\n",
        "    print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
        "    print(f\"Predicted noise shape: {predicted_noise.shape}\")\n",
        "    print(\"✓ Model forward pass successful!\")\n",
        "\n",
        "test_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf8d5851-e0fc-4926-86d5-d23e6dc11653",
      "metadata": {
        "id": "cf8d5851-e0fc-4926-86d5-d23e6dc11653"
      },
      "source": [
        "\n",
        "**Question 4.1:** What is the role of the timestep embedding in the diffusion model?\n",
        "\n",
        "Le timestep embedding informe le modèle sur l'étape de diffusion actuelle (niveau de bruit), permettant d'adapter la prédiction de bruit selon le timestep t\n",
        "\n",
        "**Question 4.2:** How text conditioning is integrated to guide the image generation process?\n",
        "\n",
        "Le texte est encodé en embeddings puis combiné spatialement avec l'image bruitée comme canaux additionnels en entrée du U-Net, guidant ainsi la génération\n",
        "\n",
        "---\n",
        "\n",
        "### ■ **<a name=\"section5\">5. Training the Model</a>** [(&#8593;)](#content)\n",
        "\n",
        "Now let's train our diffusion model on the CIFAR-10 dataset with text captions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c75f92-9f55-4618-bda4-938d8e17d962",
      "metadata": {
        "tags": [],
        "id": "55c75f92-9f55-4618-bda4-938d8e17d962"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters - reduced for quick training\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "epochs = 100  # Very few epochs for demonstration\n",
        "timesteps = 200  # Reduced timesteps for faster training (typically: 1000)\n",
        "\n",
        "# Load dataset\n",
        "dataset = CIFAR10WithCaptions(train=True, image_size=64)\n",
        "# Use a smaller subset for quick training\n",
        "subset_indices = torch.randperm(len(dataset)).to(device)[:5000]  # Only 5000 samples\n",
        "subset_dataset = torch.utils.data.Subset(dataset, subset_indices)\n",
        "dataloader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "# Initialize model with reduced timesteps\n",
        "model = TextConditionedDiffusionModel(timesteps=timesteps).to(device)\n",
        "text_encoder = SimpleTextEncoder(vocab_size=10, embed_dim=64, hidden_dim=512).to(device)\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = optim.Adam(list(model.parameters()) + list(text_encoder.parameters()), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Simplified training with proper error handling\n",
        "def train_simple_diffusion_model():\n",
        "    \"\"\"Simplified training loop for our diffusion model\"\"\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    text_encoder.train()\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (images, captions, labels) in enumerate(progress_bar):\n",
        "            images = images.to(device)\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            # Sample timesteps\n",
        "            t = model.sample_timesteps(batch_size).to(device)\n",
        "\n",
        "            # Sample noise\n",
        "            noise = torch.randn_like(images)\n",
        "\n",
        "            # Add noise to images (forward process)\n",
        "            x_t, noise = forward_diffusion_sample(images, t, model.betas, device)\n",
        "\n",
        "            # Get text embeddings\n",
        "            text_indices = labels.to(device)\n",
        "            text_embeddings = text_encoder(text_indices)\n",
        "\n",
        "            # Predict noise\n",
        "            predicted_noise = model(x_t, t, text_embeddings)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(predicted_noise, noise)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        ## show a generated sample\n",
        "        demo_generation()\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(losses)\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # return model, text_encoder\n",
        "\n",
        "# Let's also create a function to demonstrate the training concept without actual training\n",
        "def demonstrate_diffusion_concepts():\n",
        "    \"\"\"Demonstrate diffusion concepts without full training\"\"\"\n",
        "\n",
        "    print(\"=== Diffusion Model Concepts Demonstration ===\")\n",
        "\n",
        "    # 1. Show forward diffusion process\n",
        "    print(\"\\n1. Forward Diffusion Process:\")\n",
        "    visualize_forward_process()\n",
        "\n",
        "    # 2. Show model architecture\n",
        "    print(\"\\n2. Model Architecture:\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TextConditionedDiffusionModel(timesteps=200).to(device)\n",
        "    text_encoder = SimpleTextEncoder().to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters()) + sum(p.numel() for p in text_encoder.parameters())\n",
        "    print(f\"Total model parameters: {total_params:,}\")\n",
        "\n",
        "    # 3. Show dataset samples\n",
        "    print(\"\\n3. Dataset Samples:\")\n",
        "    visualize_dataset_samples()\n",
        "\n",
        "    # 4. Explain the training process\n",
        "    print(\"\\n4. Training Process Explanation:\")\n",
        "    print(\"   - Sample random timestep t\")\n",
        "    print(\"   - Add noise to image: x_t = sqrt(alpha_t) * x_0 + sqrt(1-alpha_t) * epsilon\")\n",
        "    print(\"   - Predict the noise using U-Net\")\n",
        "    print(\"   - Compute MSE loss between predicted and actual noise\")\n",
        "    print(\"   - Backpropagate and update weights\")\n",
        "\n",
        "    # 5. Show what a trained model could generate\n",
        "    print(\"\\n5. Generation Process:\")\n",
        "    print(\"   - Start from random noise x_T ~ N(0, I)\")\n",
        "    print(\"   - For t = T to 1:\")\n",
        "    print(\"     - Predict noise: epsilon_theta = model(x_t, t, text_embedding)\")\n",
        "    print(\"     - Compute x_{t-1} using the reverse process\")\n",
        "    print(\"   - Final result: x_0 (generated image)\")\n",
        "\n",
        "# Run the demonstration\n",
        "demonstrate_diffusion_concepts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Function**"
      ],
      "metadata": {
        "id": "Zjh09VDKgUNG"
      },
      "id": "Zjh09VDKgUNG"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_images(model, text_encoder, captions, num_images=4, image_size=64, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Generate images from text captions using the trained diffusion model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    text_encoder.eval()\n",
        "\n",
        "    # Convert captions to embeddings (simplified)\n",
        "    # In practice, you would use a proper text tokenizer and encoder\n",
        "    caption_indices = torch.randint(0, 10, (num_images,))  # Using class indices as proxy\n",
        "    text_embeddings = text_encoder(caption_indices.to(device))\n",
        "\n",
        "    # Start from random noise\n",
        "    x = torch.randn(num_images, 3, image_size, image_size).to(device)\n",
        "\n",
        "    # Reverse diffusion process\n",
        "    for t in tqdm(reversed(range(model.timesteps)), desc=\"Generating images\"):\n",
        "        t_batch = torch.full((num_images,), t, device=device, dtype=torch.long)\n",
        "\n",
        "        # Predict noise\n",
        "        predicted_noise = model(x, t_batch, text_embeddings)\n",
        "\n",
        "        # Get alpha and beta parameters\n",
        "        alpha_t = model.alphas[t]\n",
        "        alpha_t_cumprod = model.alphas_cumprod[t]\n",
        "        beta_t = model.betas[t]\n",
        "\n",
        "        if t > 0:\n",
        "            noise = torch.randn_like(x)\n",
        "        else:\n",
        "            noise = torch.zeros_like(x)\n",
        "\n",
        "        # Reverse process step\n",
        "        x = (1 / torch.sqrt(alpha_t)) * (\n",
        "            x - ((1 - alpha_t) / torch.sqrt(1 - alpha_t_cumprod)) * predicted_noise\n",
        "        ) + torch.sqrt(beta_t) * noise\n",
        "\n",
        "    # Denormalize images\n",
        "    images = torch.clamp(x, -1, 1)\n",
        "    images = (images + 1) / 2  # Scale to [0, 1]\n",
        "\n",
        "    return images, captions\n",
        "\n",
        "def visualize_generated_images(images, captions):\n",
        "    \"\"\"Visualize generated images with their captions\"\"\"\n",
        "    fig, axes = plt.subplots(1, len(images), figsize=(15, 3))\n",
        "\n",
        "    if len(images) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, (img, caption) in enumerate(zip(images, captions)):\n",
        "        img = img.cpu().permute(1, 2, 0)\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(caption, fontsize=10)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example generation (using a dummy model for demonstration)\n",
        "def demo_generation():\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create dummy model for demonstration\n",
        "    # In practice, you would use your trained model\n",
        "    # model = TextConditionedDiffusionModel().to(device)\n",
        "    # text_encoder = SimpleTextEncoder().to(device)\n",
        "\n",
        "    # Example captions\n",
        "    captions = [\n",
        "        \"an airplane flying in the sky\",\n",
        "        \"a cat sitting on the floor\",\n",
        "        \"a dog playing in the park\",\n",
        "        \"a ship on the ocean\"\n",
        "    ]\n",
        "\n",
        "    print(\"Generating images from text...\")\n",
        "    generated_images, _ = generate_images(\n",
        "        model, text_encoder, captions, num_images=4, device=device\n",
        "    )\n",
        "\n",
        "    visualize_generated_images(generated_images, captions)\n",
        "\n",
        "# Run the demo\n",
        "demo_generation()"
      ],
      "metadata": {
        "id": "9h-gbzmIjbiq"
      },
      "id": "9h-gbzmIjbiq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55448e68-91a5-47c4-8007-e3f7b84e5746",
      "metadata": {
        "tags": [],
        "id": "55448e68-91a5-47c4-8007-e3f7b84e5746"
      },
      "outputs": [],
      "source": [
        "## training function\n",
        "train_simple_diffusion_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60714155-c2e6-46a6-8d32-d4505ce9d5a4",
      "metadata": {
        "id": "60714155-c2e6-46a6-8d32-d4505ce9d5a4"
      },
      "source": [
        "\n",
        "**Question 5.1:** Why do we use MSE loss between predicted noise and actual noise?\n",
        "\n",
        "La MSE loss est simple et efficace pour l'apprentissage du débruitage. Elle force le modèle à prédire précisément le bruit ajouté, permettant l'inversion du processus de diffusion\n",
        "\n",
        "**Question 5.2:** What happens if we use too many or too few timesteps in the diffusion process?\n",
        "\n",
        "Lorsque c'est trop peu on a unt transition abrupte, apprentissage difficile.\n",
        "Et lorsque c'est trop on a un processus lent, et de la redondance. Le compromis optimal est 1000 timesteps.\n",
        "\n",
        "---\n",
        "\n",
        "### ■ **<a name=\"section6\">6. Text-to-Image Generation</a>** [(&#8593;)](#content)\n",
        "\n",
        "Now let's use our trained model to generate images from text descriptions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef4c0dee-0736-4e3d-bbbc-eaab9b71a1b9",
      "metadata": {
        "tags": [],
        "id": "ef4c0dee-0736-4e3d-bbbc-eaab9b71a1b9"
      },
      "outputs": [],
      "source": [
        "# Run the demo\n",
        "demo_generation()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dddea7d-d09b-4ddc-84fd-f21cffd49c55",
      "metadata": {
        "id": "9dddea7d-d09b-4ddc-84fd-f21cffd49c55"
      },
      "source": [
        "\n",
        "**Question 6.1:** Why do we start the generation process from random noise?\n",
        "\n",
        "Le processus de diffusion apprend à transformer du bruit gaussien en images. Partir du bruit aléatoire permet d'utiliser le processus inverse appris pour générer de nouvelles images.\n",
        "\n",
        "\n",
        "**Question 6.2:** What is the role of the reverse process in image generation?\n",
        "\n",
        "Le processus inverse débruite progressivement l'image en prédisant et soustrayant le bruit à chaque étape, reconstituant une image cohérente à partir du bruit initial.\n",
        "\n",
        "---\n",
        "\n",
        "### ■ **<a name=\"section7\">7. Model Evaluation</a>** [(&#8593;)](#content)\n",
        "\n",
        "Let's evaluate our model and understand its limitations and potential improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6451e8a5-f256-4719-ba72-f804abc4504d",
      "metadata": {
        "id": "6451e8a5-f256-4719-ba72-f804abc4504d"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_qualitatively(model, text_encoder, dataset, num_samples=5, device=\"cpu\"):\n",
        "    \"\"\"Qualitative evaluation by comparing generated images with real ones\"\"\"\n",
        "    model.eval()\n",
        "    text_encoder.eval()\n",
        "\n",
        "    # Get some real samples\n",
        "    real_images, real_captions, real_labels = [], [], []\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "\n",
        "    for idx in indices:\n",
        "        img, caption, label = dataset[idx]\n",
        "        real_images.append(img)\n",
        "        real_captions.append(caption)\n",
        "        real_labels.append(label)\n",
        "\n",
        "    real_images = torch.stack(real_images).to(device)\n",
        "\n",
        "    # Generate corresponding images\n",
        "    generated_images, _ = generate_images(\n",
        "        model, text_encoder, real_captions, num_images=num_samples, device=device\n",
        "    )\n",
        "\n",
        "    # Visualize comparison\n",
        "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Real image\n",
        "        real_img = real_images[i].cpu().permute(1, 2, 0)\n",
        "        real_img = (real_img * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "        axes[0, i].imshow(real_img)\n",
        "        axes[0, i].set_title(f\"Real: {real_captions[i]}\", fontsize=8)\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Generated image\n",
        "        gen_img = generated_images[i].cpu().permute(1, 2, 0)\n",
        "        axes[1, i].imshow(gen_img)\n",
        "        axes[1, i].set_title(f\"Generated: {real_captions[i]}\", fontsize=8)\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_training_components():\n",
        "    \"\"\"Analyze different components of the diffusion model\"\"\"\n",
        "\n",
        "    print(\"=== Diffusion Model Analysis ===\")\n",
        "    print(\"\\n1. Forward Process:\")\n",
        "    print(\"   - Gradually adds noise to images\")\n",
        "    print(\"   - Controlled by noise schedule (beta)\")\n",
        "    print(\"   - Results in pure Gaussian noise after T steps\")\n",
        "\n",
        "    print(\"\\n2. Reverse Process:\")\n",
        "    print(\"   - Learns to denoise images step by step\")\n",
        "    print(\"   - Uses U-Net architecture for noise prediction\")\n",
        "    print(\"   - Conditioned on timestep and text embeddings\")\n",
        "\n",
        "    print(\"\\n3. Text Conditioning:\")\n",
        "    print(\"   - Guides image generation based on text prompts\")\n",
        "    print(\"   - Uses text embeddings from encoder\")\n",
        "    print(\"   - Can be improved with cross-attention mechanisms\")\n",
        "\n",
        "    print(\"\\n4. Training Objective:\")\n",
        "    print(\"   - Predict the noise added during forward process\")\n",
        "    print(\"   - Simple MSE loss between predicted and actual noise\")\n",
        "    print(\"   - Enables stable training of diffusion models\")\n",
        "\n",
        "# Run evaluation and analysis\n",
        "analyze_training_components()\n",
        "\n",
        "# Uncomment to run qualitative evaluation (requires trained model)\n",
        "evaluate_model_qualitatively(model, text_encoder, dataset, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e27a54e-f92d-4799-911f-9a159f51d0ee",
      "metadata": {
        "id": "8e27a54e-f92d-4799-911f-9a159f51d0ee"
      },
      "source": [
        "\n",
        "**Question 7.1:** What are the main limitations of our simple diffusion model?\n",
        "\n",
        "- **Architecture:** U-Net simplifié sans attention\n",
        "- **Conditionnement:** Embedding textuel basique, pas de cross-attention\n",
        "- **Dataset:** CIFAR-10 limité (32x32, 10 classes)\n",
        "- **Entraînement:** Peu d'époques, sous-ensemble réduit\n",
        "\n",
        "\n",
        "**Question 7.2:** How could we improve the text conditioning mechanism?\n",
        "\n",
        "- **Cross-attention:** Intégrer des mécanismes d'attention croisée text-image\n",
        "- **Encodeur:** Utiliser des modèles pré-entraînés (CLIP, BERT)\n",
        "- **Architecture:** Transformer-based diffusion models\n",
        "- **Conditionnement:** Injection à différentes échelles dans le U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Demo: Image Editing using InstructPix2Pix**\n",
        "\n",
        "A demo notebook for [InstructPix2Pix](https://www.timothybrooks.com/instruct-pix2pix/) using [diffusers](https://github.com/huggingface/diffusers). InstructPix2Pix is fine-tuned stable diffusion model which allows you to edit images using language instructions.\n",
        "\n",
        "<img src='https://instruct-pix2pix.timothybrooks.com/teaser.jpg'/>"
      ],
      "metadata": {
        "id": "sEGnJXXfhBpF"
      },
      "id": "sEGnJXXfhBpF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "952eb2a5-e762-41b6-a735-54b194d97360",
      "metadata": {
        "id": "952eb2a5-e762-41b6-a735-54b194d97360"
      },
      "outputs": [],
      "source": [
        "# install package from github\n",
        "!pip install -qqq git+https://github.com/huggingface/diffusers.git gradio transformers accelerate safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To load a specific image, put its url bellow"
      ],
      "metadata": {
        "id": "gFk66VnBh5I6"
      },
      "id": "gFk66VnBh5I6"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# !wget yout_url"
      ],
      "metadata": {
        "id": "3ICn4jd_hPBL"
      },
      "id": "3ICn4jd_hPBL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import requests\n",
        "import torch\n",
        "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
        "\n",
        "model_id = \"timbrooks/instruct-pix2pix\"\n",
        "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision=\"fp16\", safety_checker=None)\n",
        "pipe.to(\"cuda\")\n",
        "pipe.enable_attention_slicing()"
      ],
      "metadata": {
        "id": "ztUWh4ZHhVEz"
      },
      "id": "ztUWh4ZHhVEz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = PIL.Image.open(\"./image.jpg\")\n",
        "image = PIL.ImageOps.exif_transpose(image)\n",
        "image = image.convert(\"RGB\")\n",
        "image"
      ],
      "metadata": {
        "id": "xaIjkZIxhXnb"
      },
      "id": "xaIjkZIxhXnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\"turn him into cyborg\",\n",
        "    \"Make it a picasso painting\",\n",
        "    \"as if it were by modigliani\",\n",
        "    \"convert to a bronze statue\",\n",
        "    \"Turn it into an anime.\",\n",
        "    \"have it look like a graphic novel\",\n",
        "    \"make him gain weight\",\n",
        "    \"what would he look like bald?\",\n",
        "    \"Have him smile\",\n",
        "    \"Put him in a cocktail party.\",\n",
        "    \"move him at the beach.\",\n",
        "    \"add dramatic lighting\",\n",
        "    \"Convert to black and white\",\n",
        "    \"What if it were snowing?\",\n",
        "    \"Give him a leather jacket\",\n",
        "    \"Turn him into a cyborg!\",\n",
        "    \"make him wear a beanie\",\n",
        "    ]\n",
        "prompt = \"Make it a picasso painting\"\n",
        "\n",
        "\n",
        "pipe(prompt, image=image, num_inference_steps=20, image_guidance_scale=1).images[0]"
      ],
      "metadata": {
        "id": "TVNmQw_Jhc_-"
      },
      "id": "TVNmQw_Jhc_-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tEo7YpkFhtCr"
      },
      "id": "tEo7YpkFhtCr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}